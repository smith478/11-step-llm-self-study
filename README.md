# 11-step-llm-self-study

The idea is to follow along with the 11-step LLM study plan from Sebastian Raschka tweeted [here](https://github.com/smith478/11-step-llm-self-study.git).

The content of the tweet is here:

A suggestion for an effective 11-step LLM summer study plan:
1) Read* Chapters 1 and 2 on implementing the data loading pipeline (https://manning.com/books/build-a-large-language-model-from-scratch & https://github.com/rasbt/LLMs-from-scratch).
2) Watch Karpathy's video on training a BPE tokenizer from scratch (https://youtube.com/watch?v=zduSFxRajkE).
3) Read Chapters 3 and 4 on implementing the model architecture.
4) Watch Karpathy's video on pretraining the LLM.
5) Read Chapter 5 on pretraining the LLM and then loading pretrained weights.
6) Read Appendix E on adding additional bells and whistles to the training loop.
7) Read Chapters 6 and 7 on finetuning the LLM.
8) Read Appendix E on parameter-efficient finetuning with LoRA.
9) Check out Karpathy's repo on coding the LLM in C code (https://github.com/karpathy/llm.c).
10) Check out LitGPT to see how multi-GPU training is implemented and how different LLM architectures compare (https://github.com/Lightning-AI/litgpt).
11) Build something cool and share it with the world.

(Read = read, run the code, and attempt the exercises ðŸ˜Š)

One interesting add-on is this [video](https://www.youtube.com/watch?v=wjZofJX0v4M&ab_channel=3Blue1Brown) from 3 Blue 1 Brown.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed010cc-126c-4fd1-b25e-03542e83970e",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "This notebook shows how text is converted to vectors representing the original text. It follows the notebook here: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/dataloader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d4556-77a7-42c9-8a25-12c2ced355fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "import torch\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef4191-c0d3-4134-8e26-1796d539f910",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a0759-1585-4c68-9bd0-b137d01e3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw text\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23fc95-b1f7-448f-8176-6e3f7f81db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea49520-bf2e-437b-ac0c-876031d9183f",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n",
    "- The following regular expression will split on whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f8682-19ec-40ab-a19f-75fded9f86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086dfff-db1e-403d-8ee1-4b93e9563c87",
   "metadata": {},
   "source": [
    "We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f51a7d-1b6a-4799-b1e3-ed2d7bfb2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b386ab9-28bf-468f-ba7b-0e2c88f4f906",
   "metadata": {},
   "source": [
    "This creates empty strings, let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a705a-eb41-4270-80c1-d698ca4f0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a911c-fa24-480d-9644-0d0bee92c0d3",
   "metadata": {},
   "source": [
    "This looks pretty good, but let's also handle other types of punctuation, such as periods, question marks, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b585ba-b389-49af-8e93-28df8b1e7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea414e02-0a11-475a-a2dc-6ac4beba0f61",
   "metadata": {},
   "source": [
    "This is pretty good, and we are now ready to apply this tokenization to the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10cc93-e120-47f1-9f9d-2bfeaa1111e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3408cc8-1a70-4f10-b52c-6ac091d82aff",
   "metadata": {},
   "source": [
    "Let's calculate the total number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90936d7b-7ff4-42a8-b3a5-2a5ee77b3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101362d-2523-466d-89af-043c9e3ddcb6",
   "metadata": {},
   "source": [
    "## Converting tokens into token IDs\n",
    "\n",
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n",
    "- From these tokens, we can now build a vocabulary that consists of all the unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72e516-1add-4b75-98fc-bb84f36c955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3f527-e169-4f45-8aa3-93af25a7920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae0864-eec1-4be4-a2bb-9c457b09e7ae",
   "metadata": {},
   "source": [
    "Below are the first 50 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf82161-8a67-41ca-9e30-8bdbcbc59e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6533b61-e236-4b47-b01e-1288db46e8a6",
   "metadata": {},
   "source": [
    "Putting it now all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf779c64-b3ff-4197-9df6-4bad5cabe4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092fc0d-182e-4f9b-90d7-3da4b3208f8e",
   "metadata": {},
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text\n",
    "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "- These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfe230-2c3c-4933-9374-5a339a499bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ab095-8520-4e49-be1c-3ba3be1d3053",
   "metadata": {},
   "source": [
    "We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803be021-cb93-4195-8c9b-96ea2691d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02258a-883a-4d91-b47a-5754b9765291",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec2ab2-d636-4f8b-a5f7-ee46445b8a4b",
   "metadata": {},
   "source": [
    "## Adding special context tokens\n",
    "\n",
    "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n",
    "- Some tokenizers use special tokens to help the LLM with additional context\n",
    "\n",
    "- Some of these special tokens are\n",
    "    - `[BOS]` (beginning of sequence) marks the beginning of text\n",
    "    - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
    "    - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "    - `[UNK]` to represent works that are not included in the vocabulary\n",
    "\n",
    "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
    "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
    "\n",
    "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "\n",
    "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
    "\n",
    "- We use the `<|endoftext|>` tokens between two independent sources of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582ddd8-7db0-4a0a-9db7-47b00f8e37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how the tokenizer handles the following text\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0334e-205b-43b3-8bbb-a1be8697e089",
   "metadata": {},
   "source": [
    "- The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
    "- To deal with such cases, we can add special tokens like `\"<|unk|>\"` to the vocabulary to represent unknown words\n",
    "- Since we are already extending the vocabulary, let's add another token called `\"<|endoftext|>\"` which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda71fd-fe55-4029-8d84-602b24447c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24080b3f-3ca5-4066-92c3-056fe4007f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882c1e1-a2f3-4ec8-beaa-2cb338eafb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20940af5-ec41-4897-a3ac-a62c0716eaf3",
   "metadata": {},
   "source": [
    "We also need to adjust the tokenizer accordingly so that it knows when and how to use the new `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6c66b-5195-4147-bbf0-8f276faea490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f57e0-4cf0-420a-a469-9d76fc53986a",
   "metadata": {},
   "source": [
    "Let's try to tokenize text with the modified tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e22a8-3d12-40c7-a1a8-e3b178c217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08d9da-0c51-468a-8d0a-7430cbf077da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a0431-9d9e-4dd7-8a37-7433d5c90d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d5e00-37b8-4964-9183-67847ce1dfd1",
   "metadata": {},
   "source": [
    "## BytePair encoding\n",
    "\n",
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dae311-ca45-4a9c-86d0-297896f74a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0143d64-b599-4097-aabe-75b1e76749d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b912fb4-1f16-4e68-87df-a04744d9a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3c6dc-3043-48c4-aaf8-a11ecf344bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fbdd2-5df3-4316-9b07-0528f3b03225",
   "metadata": {},
   "source": [
    "BPE tokenizers break down unknown words into subwords and individual characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c01212-0ea2-486b-b178-acd10c0b718f",
   "metadata": {},
   "source": [
    "## Data sampling with a sliding window\n",
    "\n",
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1c9c3-36dc-4a24-8eb7-6cda82e4f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ce2af-fa25-4ade-b84c-1d3136b18ca6",
   "metadata": {},
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fef05-2450-446b-b707-75e13b1992f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15d080-5477-4e4c-bc4d-ee0f85c168a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2c1ae-2742-480e-9b84-f7f10f58f16d",
   "metadata": {},
   "source": [
    "One by one, the prediction would look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7374a1e-2b21-42ae-9eb2-9094808cf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1810150-d2ff-42dd-99bb-bd62ff6f5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a035494-a138-487d-80a1-07a54ba31ac1",
   "metadata": {},
   "source": [
    "- We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
    "- For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80756480-8d67-452b-8393-554d36f1c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98d24c-28d5-4fb4-a0a6-e6d125022b78",
   "metadata": {},
   "source": [
    "- We use a sliding window approach, changing the position by +1\n",
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b795f-38df-45f1-b1df-d340719c67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eae2a5-9eac-4c63-a9ab-1317c8d5ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8cb005-a1a4-4d40-bb88-55d6db814e49",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a53f1-e258-41a4-a033-a5c4de266c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fe158-426c-43e8-a635-5b90c992b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab79bf-8b89-4b1a-8b76-590f73e57c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b5cc3f-fd1c-4350-8081-0d22786e7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da1d3c-ebfe-4d27-ac4f-0a92100510a7",
   "metadata": {},
   "source": [
    "## Creating token embeddings\n",
    "\n",
    "- The data is already almost ready for an LLM\n",
    "- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ed2ad-4e57-4d0d-8344-85375c51d250",
   "metadata": {},
   "source": [
    "Suppose we have the following four input examples with input ids 5, 1, 3, and 2 (after tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da6558-75c9-4f66-949f-e6bfef59b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7064e8d-34cb-4ac4-9c0f-380cc0e6c038",
   "metadata": {},
   "source": [
    "For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9322fac-4a30-4e7a-a482-e15265e535bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27ac82-80a7-4f97-be71-cd72f86a4464",
   "metadata": {},
   "source": [
    "This would result in a 6x3 weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5896a-e053-4585-997f-5e7091259c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a23a7-6da5-44d0-98c0-2ff41d5d59ac",
   "metadata": {},
   "source": [
    "- For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer.\n",
    "- Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation\n",
    "- To convert a token with id 3 into a 3-dimensional vector, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bab93-6365-4d75-966c-afbef4f0709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9cace-46d5-4b1a-bd63-1daf9e4a87ca",
   "metadata": {},
   "source": [
    "- Note that the above is the 4th row in the embedding_layer weight matrix\n",
    "- To embed all four input_ids values above, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd30442-22e4-4c08-b8c7-53d084693a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ac245-d30e-4565-8ea5-8a4fff2624ae",
   "metadata": {},
   "source": [
    "An embedding layer is essentially a look-up operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ce8ca-536c-4cc7-b451-ec9c77083661",
   "metadata": {},
   "source": [
    "## Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1618af-dc81-4c9c-bcb2-28334cece7b4",
   "metadata": {},
   "source": [
    "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence\n",
    "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model\n",
    "- The BytePair encoder has a vocabulary size of 50,257\n",
    "- Suppose we want to encode the input tokens into a 256-dimensional vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9470e8-609b-410e-953b-e6e977a0bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda18fe0-9b90-43e9-aa68-dc651c7cef55",
   "metadata": {},
   "source": [
    "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "- If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb465c-2bc0-42be-ba95-0607f9eff1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ec3c7-841f-43d9-a21c-aa5ad89ea617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4f85a-2140-4c90-9b9a-3c872279ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a2390-c63a-4f05-9fbe-3fd41018f7c6",
   "metadata": {},
   "source": [
    "GPT-2 uses absolute position embeddings, so we just create another embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bc93a-3500-4ce1-98ce-47f308446b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d56afb-fae5-44ab-9a19-86e2cd953650",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c003320-6058-4dd5-97f3-fe8c17e8a9d2",
   "metadata": {},
   "source": [
    "To create the input embeddings used in an LLM, we simply add the token and the positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df00d4-5e1a-477c-be65-4b1bf288cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2778314-1d19-42dd-b693-afbbd2d7ac05",
   "metadata": {},
   "source": [
    "- In the initial phase of the input processing workflow, the input text is segmented into separate tokens\n",
    "- Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d45b56-da0c-4eaf-9b37-dabb41829fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
